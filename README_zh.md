<div align="center">

<!-- <!-- <h1 style="color: #33A6B8; font-family: Helvetica"> OmniLMM </h1> -->

<img src="./assets/title-2.png" width="200em" ></img> 

**æ€§èƒ½å¼ºå¤§ä¸”éƒ¨ç½²é«˜æ•ˆçš„å¤šæ¨¡æ€å¤§æ¨¡å‹**
<p align="center">
  OmniLMM-3B  <a href="https://huggingface.co/openbmb/MiniCPM-V/">ğŸ¤—</a> <a href="http://120.92.209.146:80/">ğŸ¤–</a> |
  OmniLMM-12B <a href="https://huggingface.co/openbmb/OmniLMM-12B/">ğŸ¤—</a> <a href="http://120.92.209.146:8081">ğŸ¤–</a>
</p>

</div>


**OmniLMM** æ˜¯ä¸€ç³»åˆ—å–„äºå¤„ç†å›¾æ–‡è¾“å…¥çš„å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚è¯¥ç³»åˆ—æ¨¡å‹æ¥å—å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶æä¾›é«˜è´¨é‡çš„æ–‡æœ¬è¾“å‡ºã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªç‰ˆæœ¬çš„ OmniLMMï¼Œæ—¨åœ¨å®ç°**å¼ºå¤§çš„æ€§èƒ½å’Œé«˜æ•ˆçš„éƒ¨ç½²**ï¼š

- **OmniLMM-12B**ï¼šç›¸æ¯”åŒè§„æ¨¡å…¶ä»–æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å…·æœ‰é¢†å…ˆæ€§èƒ½ã€‚

- **OmniLMM-3B**ï¼šå¯åœ¨ç»ˆç«¯è®¾å¤‡ä¸Šéƒ¨ç½²å¹¶å…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€å¯¹è¯èƒ½åŠ›ã€‚

[English Document](./README.md)

## ç›®å½•
- [OmniLMM-12B](#omnilmm-12b)
- [OmniLMM-3B](#omnilmm-3b)
- [ä½“éªŒ](#demo)
- [å®‰è£…](#install)
- [æ¨ç†](#inference)
- [æ¨¡å‹åº“](#model-zoo)

## OmniLMM-12B
**OmniLMM-12B** æ˜¯å½“å‰ç³»åˆ—ä¸­æ€§èƒ½æœ€å¼ºå¤§çš„ç‰ˆæœ¬ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ä¸€ä¸ªæ„ŸçŸ¥é‡é‡‡æ ·å±‚è¿æ¥ EVA02-5B å’Œ Zephyr-7B-Î² æ¥æ„å»ºï¼Œé‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹å…·æœ‰ä¸‰ä¸ªæ˜¾è‘—ç‰¹å¾ï¼š

- ğŸ”¥ **å“è¶Šæ€§èƒ½ã€‚**

  OmniLMM-12B ç›¸æ¯”å…¶ä»–åŒè§„æ¨¡æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—**é¢†å…ˆçš„æ€§èƒ½**ï¼ˆåŒ…æ‹¬ MMEã€MMBenchã€SEED-Bench ç­‰ï¼‰ã€‚æ¨¡å‹æŒæ¡äº†**ä¸°å¯Œçš„å¤šæ¨¡æ€ä¸–ç•ŒçŸ¥è¯†**ã€‚

- ğŸ† **å¯ä¿¡è¡Œä¸ºã€‚**

  LMMs çš„å¹»è§‰é—®é¢˜å¤‡å—å…³æ³¨ï¼Œæ¨¡å‹ç»å¸¸ç”Ÿæˆå’Œå›¾åƒä¸­çš„äº‹å®ä¸ç¬¦çš„æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œç¡®ä¿¡åœ°æè¿°å›¾ç‰‡ä¸­å¹¶ä¸å­˜åœ¨çš„ç‰©ä½“ï¼‰ã€‚OmniLMM-12Bæ˜¯ **ç¬¬ä¸€ä¸ªé€šè¿‡å¤šæ¨¡æ€ RLHF å¯¹é½çš„ç»¼åˆèƒ½åŠ›ä¼˜ç§€çš„å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹**ï¼ˆé€šè¿‡æˆ‘ä»¬æœ€è¿‘æå‡ºçš„ [RLHF-V](https://rlhf-v.github.io/) æŠ€æœ¯ï¼‰ã€‚è¯¥æ¨¡å‹åœ¨ [MMHal-Bench](https://huggingface.co/datasets/Shengcao1006/MMHal-Bench) å¹»è§‰è¯„æµ‹åŸºå‡†ä¸Šä½åˆ—å¼€æºæ¨¡å‹ä¸­**ç¬¬ä¸€**ï¼Œå¹¶åœ¨ [Object HalBench](https://arxiv.org/abs/2312.00849) ä¸­**è¶…è¿‡äº† GPT-4V**ã€‚

- ğŸ•¹ **å®æ—¶å¤šæ¨¡æ€äº¤äº’ã€‚**

  æˆ‘ä»¬å°† OmniLMM-12B å’Œ GPT-3.5 ç»“åˆæˆä¸€ä¸ª**å®æ—¶å¤šæ¨¡æ€äº¤äº’åŠ©æ‰‹**ã€‚è¯¥åŠ©æ‰‹æ¥å—æ¥è‡ªç›¸æœºçš„è§†é¢‘æµå’Œæ¥è‡ªéº¦å…‹é£çš„è¯­éŸ³æµï¼Œå¹¶å‘å‡ºè¯­éŸ³è¾“å‡ºã€‚è™½ç„¶è¿˜å¤„äºåˆçº§é˜¶æ®µï¼Œä½†æˆ‘ä»¬ä¹Ÿå‘ç°è¯¥æ¨¡å‹**æ— éœ€è§†é¢‘ç¼–è¾‘**å°±å¯ä»¥**å¤ç°å‡ºç°åœ¨ Gemini æ¼”ç¤ºè§†é¢‘ä¸­çš„ä¸€äº›æœ‰è¶£ä¾‹å­**ã€‚

### æ€§èƒ½è¯„ä¼°

<table>
<thead>
  <tr>
    <th align="left">Model</th>
    <th>Size</th>
    <th>MME</th>
    <th nowrap="nowrap">MMB dev (en)</th>
    <th nowrap="nowrap" >MMMU val</th>
    <th nowrap="nowrap" >MMHal-Bench</th>
    <th nowrap="nowrap" >Object HalBench</th>
    <th nowrap="nowrap" >SeedBench-I</th>
    <th>MathVista</th>
    <th nowrap="nowrap" >LLaVA Bench W</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td align="left">GPT-4Vâ€ </td>
    <td>-</td>
    <td>1409</td>
    <td>75.1 </td>
    <td>56.8</td>
    <td>3.53 / 70.8</td>
    <td>86.4 / 92.7</td>
    <td>71.6 </td>
    <td>47.8 </td>
    <td>93.1 </td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left">Qwen-VL-Plusâ€ </td>
    <td>-</td>
    <td>1681</td>
    <td>66.2 </td>
    <td>45.2</td>
    <td>- </td>
    <td>- </td>
    <td>65.7 </td>
    <td>36.0 </td>
    <td>73.7 </td>
  </tr>
  <tr>
    <td align="left">Yi-VL 6B</td>
    <td align="right">6.7B </td>
    <td>- </td>
    <td>68.2 </td>
    <td>39.1 </td>
    <td>- </td>
    <td>- </td>
    <td>66.1 </td>
    <td>28.0 </td>
    <td>39.9 </td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" >Qwen-VL-Chat</td>
    <td align="right">9.6B</td>
    <td>1488</td>
    <td>60.6 </td>
    <td>35.9</td>
    <td>2.93 / 59.4</td>
    <td>56.2 / 80.0</td>
    <td>64.8 </td>
    <td>33.8 </td>
    <td>67.7 </td>
  </tr>
  <tr>
    <td align="left" >CogVLM</td>
    <td align="right">17.4B</td>
    <td>1438</td>
    <td>63.7 </td>
    <td>32.1 </td>
    <td>2.68 / 52.1 </td>
    <td>73.6 / 87.4 </td>
    <td>68.8 </td>
    <td>34.7 </td>
    <td>73.9 </td>
  </tr>
  <tr>
    <td align="left" >LLaVA 1.5</td>
    <td align="right">13.6B </td>
    <td>1531 </td>
    <td>68.2 </td>
    <td>36.4 </td>
    <td>2.71 / 51.0 </td>
    <td>53.7 / 77.4 </td>
    <td>68.1 </td>
    <td>26.4 </td>
    <td>64.6 </td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" ><b>OmniLMM-12B</b></td>
    <td align="right">11.6B </td>
    <td>1637 </td>
    <td>71.6 </td>
    <td>40.7 </td>
    <td>3.45 / 68.8 </td>
    <td>90.3 / 95.5 </td>
    <td>71.1 </td>
    <td>34.9 </td>
    <td>72.0 </td>
  </tr>
</tbody>
</table>
<small>â€ : é—­æºæ¨¡å‹</small>


## OmniLMM-3B

**OmniLMM-3B**ï¼ˆå³ MiniCPM-Vï¼‰æ˜¯ä¸€ç§æˆ‘ä»¬çš„é«˜æ•ˆç‡ç‰ˆæœ¬æ¨¡å‹ï¼Œå¯ç”¨äºç»ˆç«¯æœºå™¨ä¸Šçš„éƒ¨ç½²ã€‚è¯¥æ¨¡å‹åŸºäº SigLip-400M å’Œ MiniCPM-2.4B æ„å»ºï¼Œé€šè¿‡æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨è¿æ¥ã€‚OmniLMM-3Bçš„æ˜¾è‘—ç‰¹ç‚¹åŒ…æ‹¬ï¼š

- âš¡ï¸ **é«˜æ•ˆç‡ã€‚**

  OmniLMM-3B å¯ä»¥**é«˜æ•ˆåœ°éƒ¨ç½²åœ¨å¤§å¤šæ•°GPUå¡å’Œä¸ªäººç”µè„‘ä¸Š**ï¼Œç”šè‡³**åœ¨ç§»åŠ¨æ‰‹æœºç­‰ç»ˆç«¯è®¾å¤‡ä¸Š**ã€‚åœ¨è§†è§‰ç¼–ç æ–¹é¢ï¼Œæˆ‘ä»¬é€šè¿‡æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨å°†å›¾åƒè¡¨ç¤ºå‹ç¼©ä¸º 64 ä¸ª tokenï¼Œè¿œè¿œå°‘äºåŸºäºMLPæ¶æ„çš„å…¶ä»–LMMsï¼ˆé€šå¸¸å¤§äº 512 ä¸ª tokenï¼‰ã€‚è¿™ä½¿å¾— OmniLMM-3B åœ¨æ¨ç†æœŸé—´**å†…å­˜æˆæœ¬æ›´ä½ä¸”é€Ÿåº¦æ›´å¿«**ã€‚

- ğŸ”¥ **ä¼˜ç§€çš„æ€§èƒ½ã€‚**

  OmniLMM-3B åœ¨ä¸ç›¸ä¼¼å¤§å°æ¨¡å‹ç›¸æ¯”çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†**æœ€å…ˆè¿›çš„æ€§èƒ½**ï¼Œè¶…è¿‡äº†åŸºäº Phi-2æ„å»ºçš„ç°æœ‰ LMMsã€‚å®ƒç”šè‡³**å®ç°äº†ä¸9.6B Qwen-VL-Chat ç›¸åª²ç¾æˆ–æ›´å¥½çš„æ€§èƒ½**ã€‚

- ğŸ™Œ **åŒè¯­æ”¯æŒã€‚**

  OmniLMM-3B æ˜¯**ç¬¬ä¸€ä¸ªæ”¯æŒè‹±è¯­å’Œä¸­æ–‡åŒè¯­å¤šæ¨¡æ€äº¤äº’çš„ç»ˆç«¯å¯éƒ¨ç½² LMM**ã€‚è¿™æ˜¯é€šè¿‡è·¨è¯­è¨€æ³›åŒ–å¤šæ¨¡æ€èƒ½åŠ›å®ç°çš„ï¼Œè¿™æ˜¯æˆ‘ä»¬ ICLR 2024 [spotlight è®ºæ–‡](https://arxiv.org/abs/2308.12038)ä¸­çš„ä¸€é¡¹æŠ€æœ¯ã€‚

### Evaluation

<div align="center">

<table style="margin: 0px auto;">
<thead>
  <tr>
    <th align="left">Model</th>
    <th>Size</th>
    <th>MME</th>
    <th nowrap="nowrap" >MMB dev (en)</th>
    <th nowrap="nowrap" >MMB dev (zh)</th>
    <th nowrap="nowrap" >MMMU val</th>
    <th nowrap="nowrap" >CMMMU val</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td align="left">LLaVA-Phi</td>
    <td align="right">3B</td>
    <td>1335</td>
    <td>59.8</td>
    <td>- </td>
    <td>- </td>
    <td>- </td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left">MobileVLM</td>
    <td align="right">3B</td>
    <td>1289</td>
    <td>59.6</td>
    <td>- </td>
    <td>- </td>
    <td>- </td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" >Imp-v1</td>
    <td align="right">3B</td>
    <td>1434</td>
    <td>66.5</td>
    <td>- </td>
    <td>- </td>
    <td>- </td>
  </tr>
  <tr>
    <td align="left" >Qwen-VL-Chat</td>
    <td align="right" >9.6B</td>
    <td>1487</td>
    <td>60.6 </td>
    <td>56.7 </td>
    <td>35.9 </td>
    <td>30.7 </td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" >CogVLM</td>
    <td align="right">17.4B </td>
    <td>1438 </td>
    <td>63.7 </td>
    <td>53.8 </td>
    <td>32.1 </td>
    <td>- </td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" ><b>OmniLMM-3B</b></td>
    <td align="right">3B </td>
    <td>1452 </td>
    <td>67.3 </td>
    <td>61.9 </td>
    <td>34.7 </td>
    <td>32.1 </td>
  </tr>
</tbody>
</table>

</div>

### æ ·ä¾‹å±•ç¤º

<table align="center" >
  <p align="center" > 
    <img src="assets/Snake_cn_Mushroom_en.gif" width=48%/>
  </p>
</table>

## ä½“éªŒ

ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥å°è¯•ä½¿ç”¨æˆ‘ä»¬çš„ç½‘é¡µç«¯æ¨ç†æœåŠ¡ï¼š [OmniLMM-12B](http://120.92.209.146:8081) ï½œ [OmniLMM-3B](http://120.92.209.146:80).

## å®‰è£…

1. Clone this repository and navigate to the source folder

```bash
git clone https://github.com/OpenBMB/OmniLMM.git
cd OmniLMM
```

2. Create conda environment

```Shell
conda create -n OmniLMM python=3.10 -y
conda activate OmniLMM
```

3. Install dependencies

```shell
pip install -r requirements.txt
```

## æ¨ç†

### æ¨¡å‹åº“

| æ¨¡å‹                | ç®€ä»‹       | ä¸‹è½½é“¾æ¥ |
|:----------------------|:-------------------|:---------------:|
| OmniLMM-12B | æ›´å¼ºå¤§çš„æ€§èƒ½è¡¨ç°                   |  [ğŸ¤—](https://huggingface.co/openbmb/OmniLMM-12B) &nbsp;&nbsp; <a url="https://modelscope.cn/models/OpenBMB/OmniLMM-12B/files"> <img src="./assets/modelscope_logo.png" width="20px"></img></a> |
| OmniLMM-3B  | æ”¯æŒç»ˆç«¯è®¾å¤‡ä¸Šçš„é«˜æ•ˆéƒ¨ç½²ï¼Œæ€§èƒ½ä¼˜ç§€          |  [ğŸ¤—](https://huggingface.co/openbmb/MiniCPM-V) &nbsp;&nbsp; <a url="https://modelscope.cn/models/OpenBMB/MiniCPM-V/files"> <img src="./assets/modelscope_logo.png" width="20px"></img></a> |


### å¤šè½®å¯¹è¯

è¯·å‚è€ƒä»¥ä¸‹ä»£ç è¿è¡Œ  `OmniLMM` çš„æ¨ç†æœåŠ¡ã€‚

<div align="center">
<img src="assets/COCO_test2015_000000262144.jpg" width="660px">
</div>

##### OmniLMM-12B

```python
from chat import OmniLMMChat, img2base64

chat_model = OmniLMMChat('openbmb/OmniLMM-12B')

im_64 = img2base64('./assets/COCO_test2015_000000262144.jpg')

# First round chat 
msgs = [{"role": "user", "content": "What are the people doing?"}]

inputs = {"image": im_64, "question": json.dumps(msgs)}
answer = chat_model.process(inputs)
print(answer)

# Second round chat 
# pass history context of multi-turn conversation
msgs.append({"role": "assistant", "content": answer})
msgs.append({"role": "user", "content": "Describe the image"})

inputs = {"image": im_64, "question": json.dumps(msgs)}
answer = chat_model.process(inputs)
print(answer)
```

We can obtain the following results:
```
"The people in the image are playing baseball. One person is pitching a ball, another one is swinging a bat to hit it, and there's also an umpire present who appears to be watching the game closely."

"The image depicts a baseball game in progress. A pitcher is throwing the ball, while another player is swinging his bat to hit it. An umpire can be seen observing the play closely."
```

##### OmniLMM-3B
```python
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model_path='openbmb/MiniCPM-V'
model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model.eval().cuda()

image = Image.open('./assets/COCO_test2015_000000262144.jpg').convert('RGB')

question = 'è¯·æè¿°ä¸€ä¸‹è¯¥å›¾åƒ'
res, context, _ = model.chat(
    image=image,
    question=question,
    context=None,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.7
)
print(res)
```

## âœ… æœªæ¥è®¡åˆ’

- [ ] æ”¯æŒæ¨¡å‹å¾®è°ƒ
- [ ] æœ¬åœ°å¯è§†åŒ–éƒ¨ç½²
- [ ] å®æ—¶å¤šæ¨¡æ€äº¤äº’ä»£ç å¼€æº
- [ ] æ›´æ–° OCR èƒ½åŠ›å¢å¼ºç‰ˆæœ¬


## æ¨¡å‹åè®®

æœ¬ä»“åº“ä¸­ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æº

OmniLMMs æ¨¡å‹æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª â€œ[é€šç”¨æ¨¡å‹è®¸å¯åè®®-æ¥æºè¯´æ˜-å®£ä¼ é™åˆ¶-å•†ä¸šæˆæƒ](https://github.com/OpenBMB/General-Model-License/blob/main/é€šç”¨æ¨¡å‹è®¸å¯åè®®-æ¥æºè¯´æ˜-å®£ä¼ é™åˆ¶-å•†ä¸šæˆæƒ.md)â€ã€‚

OmniLMMs æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ã€‚

å¦‚éœ€å°†æ¨¡å‹ç”¨äºå•†ä¸šç”¨é€”ï¼Œè¯·è”ç³» cpm@modelbest.cn æ¥è·å–ä¹¦é¢æˆæƒï¼Œåœ¨ç™»è®°åäº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚


## å£°æ˜

ä½œä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ŒOmniLMMs é€šè¿‡å­¦ä¹ å¤§é‡çš„å¤šæ¨¡æ€è¯­æ–™æ¥ç”Ÿæˆå†…å®¹ï¼Œä½†å®ƒæ— æ³•ç†è§£ã€è¡¨è¾¾ä¸ªäººè§‚ç‚¹æˆ–ä»·å€¼åˆ¤æ–­ï¼Œå®ƒæ‰€è¾“å‡ºçš„ä»»ä½•å†…å®¹éƒ½ä¸ä»£è¡¨æ¨¡å‹å¼€å‘è€…çš„è§‚ç‚¹å’Œç«‹åœºã€‚

å› æ­¤ç”¨æˆ·åœ¨ä½¿ç”¨ OmniLMMs ç”Ÿæˆçš„å†…å®¹æ—¶ï¼Œåº”è‡ªè¡Œè´Ÿè´£å¯¹å…¶è¿›è¡Œè¯„ä¼°å’ŒéªŒè¯ã€‚

å¦‚æœç”±äºä½¿ç”¨ OmniLMMs å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚


## ğŸ« æœºæ„

æœ¬é¡¹ç›®ç”±ä»¥ä¸‹æœºæ„å…±åŒå¼€å‘ï¼š

- <img src="assets/thunlp.png" width="28px"> [æ¸…åå¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†å®éªŒå®¤](https://nlp.csai.tsinghua.edu.cn/)
- <img src="assets/modelbest.png" width="28px"> [é¢å£æ™ºèƒ½](https://modelbest.cn/)
- <img src="assets/zhihu.webp" width="28px"> [çŸ¥ä¹](https://www.zhihu.com/ )

